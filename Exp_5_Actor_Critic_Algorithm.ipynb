{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Environment setup\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "num_actions = env.action_space.n\n",
        "num_states = env.observation_space.shape[0]\n",
        "\n",
        "# Hyperparameters\n",
        "gamma = 0.99  # Discount factor\n",
        "lr_actor = 0.001\n",
        "lr_critic = 0.005\n",
        "\n",
        "# Actor network\n",
        "actor_model = tf.keras.Sequential([\n",
        "    layers.Input(shape=(num_states,)),\n",
        "    layers.Dense(24, activation='relu'),\n",
        "    layers.Dense(24, activation='relu'),\n",
        "    layers.Dense(num_actions, activation='softmax')\n",
        "])\n",
        "\n",
        "# Critic network\n",
        "critic_model = tf.keras.Sequential([\n",
        "    layers.Input(shape=(num_states,)),\n",
        "    layers.Dense(24, activation='relu'),\n",
        "    layers.Dense(24, activation='relu'),\n",
        "    layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Optimizers\n",
        "actor_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_actor)\n",
        "critic_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_critic)\n",
        "\n",
        "# Training function\n",
        "def train():\n",
        "    for episode in range(50):\n",
        "        state = env.reset()\n",
        "        state = np.reshape(state, [1, num_states])\n",
        "        episode_reward = 0\n",
        "\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            while True:\n",
        "                # Select action\n",
        "                state_tensor = tf.convert_to_tensor(state)\n",
        "                action_probs = actor_model(state_tensor)\n",
        "                action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
        "\n",
        "                # Execute action\n",
        "                next_state, reward, done, _ = env.step(action)\n",
        "                next_state = np.reshape(next_state, [1, num_states])\n",
        "\n",
        "                # Compute TD target and error\n",
        "                state_value = critic_model(state_tensor)\n",
        "                next_state_value = critic_model(tf.convert_to_tensor(next_state))\n",
        "                td_target = reward + gamma * next_state_value * (1 - int(done))\n",
        "                td_error = td_target - state_value\n",
        "\n",
        "                # Actor loss\n",
        "                action_one_hot = tf.one_hot([action], num_actions)\n",
        "                log_prob = tf.math.log(tf.reduce_sum(action_probs * action_one_hot, axis=1))\n",
        "                actor_loss = -log_prob * tf.stop_gradient(td_error)\n",
        "\n",
        "                # Critic loss\n",
        "                critic_loss = tf.square(td_error)\n",
        "\n",
        "                # Update networks\n",
        "                actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
        "                critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
        "                actor_optimizer.apply_gradients(zip(actor_grad, actor_model.trainable_variables))\n",
        "                critic_optimizer.apply_gradients(zip(critic_grad, critic_model.trainable_variables))\n",
        "\n",
        "                episode_reward += reward\n",
        "                state = next_state\n",
        "\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "        print(f\"Episode: {episode}, Reward: {episode_reward}\")\n",
        "\n",
        "train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLm6ZteVbCie",
        "outputId": "6ebfd964-2e7f-4984-ad8e-beef196bbd2d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0, Reward: 17.0\n",
            "Episode: 1, Reward: 13.0\n",
            "Episode: 2, Reward: 15.0\n",
            "Episode: 3, Reward: 30.0\n",
            "Episode: 4, Reward: 36.0\n",
            "Episode: 5, Reward: 14.0\n",
            "Episode: 6, Reward: 13.0\n",
            "Episode: 7, Reward: 17.0\n",
            "Episode: 8, Reward: 36.0\n",
            "Episode: 9, Reward: 41.0\n",
            "Episode: 10, Reward: 29.0\n",
            "Episode: 11, Reward: 19.0\n",
            "Episode: 12, Reward: 12.0\n",
            "Episode: 13, Reward: 22.0\n",
            "Episode: 14, Reward: 28.0\n",
            "Episode: 15, Reward: 34.0\n",
            "Episode: 16, Reward: 13.0\n",
            "Episode: 17, Reward: 14.0\n",
            "Episode: 18, Reward: 18.0\n",
            "Episode: 19, Reward: 14.0\n",
            "Episode: 20, Reward: 47.0\n",
            "Episode: 21, Reward: 20.0\n",
            "Episode: 22, Reward: 63.0\n",
            "Episode: 23, Reward: 30.0\n",
            "Episode: 24, Reward: 17.0\n",
            "Episode: 25, Reward: 35.0\n",
            "Episode: 26, Reward: 20.0\n",
            "Episode: 27, Reward: 30.0\n",
            "Episode: 28, Reward: 26.0\n",
            "Episode: 29, Reward: 15.0\n",
            "Episode: 30, Reward: 60.0\n",
            "Episode: 31, Reward: 37.0\n",
            "Episode: 32, Reward: 14.0\n",
            "Episode: 33, Reward: 30.0\n",
            "Episode: 34, Reward: 32.0\n",
            "Episode: 35, Reward: 17.0\n",
            "Episode: 36, Reward: 28.0\n",
            "Episode: 37, Reward: 37.0\n",
            "Episode: 38, Reward: 14.0\n",
            "Episode: 39, Reward: 22.0\n",
            "Episode: 40, Reward: 42.0\n",
            "Episode: 41, Reward: 15.0\n",
            "Episode: 42, Reward: 38.0\n",
            "Episode: 43, Reward: 21.0\n",
            "Episode: 44, Reward: 23.0\n",
            "Episode: 45, Reward: 20.0\n",
            "Episode: 46, Reward: 44.0\n",
            "Episode: 47, Reward: 30.0\n",
            "Episode: 48, Reward: 44.0\n",
            "Episode: 49, Reward: 57.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f8MAO3n6cBx4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}